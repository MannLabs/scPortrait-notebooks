{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafedb9a-cf4f-4500-aa14-edbb19540de0",
   "metadata": {},
   "source": [
    "# Example Notebook Running scPortrait on datasets with multiple images\n",
    "\n",
    "The scPortrait base Project class is focused on processing whole slide images. In case you want to process Image Datasets with multiple-images then you will need to use the scPortrait TimecourseProject class and associated methods. This tutorial will walk you through this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e7032-5134-4a41-b9f6-5f0f62f82b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scportrait.pipeline.extraction import TimecourseHDF5CellExtraction\n",
    "from scportrait.pipeline.project import TimecourseProject\n",
    "from scportrait.pipeline.segmentation.workflows import Multithreaded_Cytosol_Cellpose_TimecourseSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269ba93-a4f8-4e4c-aaa3-6523695e8004",
   "metadata": {},
   "source": [
    "## Some Helper Functions for visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c5d00-4cde-4590-98a0-16f0bcfd0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scportrait.processing.preprocessing import percentile_normalization\n",
    "\n",
    "\n",
    "def plot_seg_overlay_timecourse(project_path, channel_to_show=0, seg_mask_to_show=0, selection=None, return_fig=False):\n",
    "    seg_path = f\"{project_path}/segmentation/input_segmentation.h5\"\n",
    "    with h5py.File(seg_path, \"r\") as hf:\n",
    "        segmentation = hf.get(\"segmentation\")\n",
    "        channels = hf.get(\"input_images\")\n",
    "\n",
    "        print(segmentation)\n",
    "        print(channels)\n",
    "\n",
    "        if selection is None:\n",
    "            segmentation = segmentation[0, seg_mask_to_show, :, :]\n",
    "            image = channels[0, channel_to_show, :, :]\n",
    "\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(percentile_normalization(image), cmap=\"Greys_r\")\n",
    "            plt.imshow(segmentation, alpha=0.5, cmap=\"jet\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        if return_fig:\n",
    "            return fig\n",
    "\n",
    "\n",
    "def visualize_single_cells(project_path, n_cells=10):\n",
    "    cells_path = f\"{project_path}/extraction/data/single_cells.h5\"\n",
    "\n",
    "    with h5py.File(cells_path, \"r\") as hf:\n",
    "        cells = hf.get(\"single_cell_data\")\n",
    "        n_channels = cells.shape[1]\n",
    "\n",
    "        fig, axs = plt.subplots(n_cells, n_channels, figsize=(n_channels * 1, n_cells * 1))\n",
    "\n",
    "        for i in range(n_cells):\n",
    "            image = cells[i]\n",
    "            for n in range(n_channels):\n",
    "                axs[i, n].imshow(image[n])\n",
    "                axs[i, n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565c7e0-f6d4-46c0-b81e-5c1f362a4b66",
   "metadata": {},
   "source": [
    "## Initialize a Project\n",
    "\n",
    "scPortrait works with a project structure that is kept the same across different projects. Each project contains all of the results from one run. Each Project has the same general structure:\n",
    "\n",
    "    .\n",
    "    ├── classification\n",
    "    │   └── classifier_name\n",
    "    │       └── processing.log\n",
    "    ├── config.yml\n",
    "    ├── extraction\n",
    "    ├── segmentation\n",
    "    └── processing.log\n",
    "\n",
    "\n",
    "At the beginning of a scPortrait analysis a new project is generated where configuration parameters are loaded from the config file. This determines how each processing step will be executed.Here is a minimal example including the different cellpose segmentation methods for TimecourseProjects.\n",
    "\n",
    "    ---\n",
    "    name: \"Example Timecourse Project\"\n",
    "    input_channels: 3\n",
    "    output_channels: 5\n",
    "    Cytosol_Cellpose_TimecourseSegmentation:\n",
    "        input_channels: 3\n",
    "        output_masks: 2\n",
    "        shard_size: 4000000 # Average number of pixel per tile. 10.000 * 10.000 pixel are recommended. Can be adapted to memory and computation needs. \n",
    "        chunk_size: 50 # chunk size for chunked HDF5 storage. is needed for correct caching and high performance reading. should be left at 50.\n",
    "        cache: \".\"\n",
    "        lower_quantile_normalization:   0.001\n",
    "        upper_quantile_normalization:   0.999\n",
    "        median_filter_size:   4 # Size in pixels\n",
    "        nucleus_segmentation:\n",
    "            model: \"nuclei\"\n",
    "        cytosol_segmentation:\n",
    "            model: \"cyto2\"\n",
    "        chunk_size: 50\n",
    "        match_masks: True\n",
    "        filtering_threshold_mask_matching: 0.95\n",
    "        filter_masks_size: False\n",
    "    Multithreaded_Cytosol_Cellpose_TimecourseSegmentation:\n",
    "        input_channels: 3\n",
    "        output_masks: 2\n",
    "        shard_size: 4000000 # Average number of pixel per tile. 10.000 * 10.000 pixel are recommended. Can be adapted to memory and computation needs. \n",
    "        chunk_size: 50 # chunk size for chunked HDF5 storage. is needed for correct caching and high performance reading. should be left at 50.\n",
    "        threads: 5 # number of shards / tiles segmented at the same size. should be adapted to the maximum amount allowed by memory.\n",
    "        cache: \".\"\n",
    "        lower_quantile_normalization:   0.001\n",
    "        upper_quantile_normalization:   0.999\n",
    "        median_filter_size:   4 # Size in pixels\n",
    "        nucleus_segmentation:\n",
    "            model: \"nuclei\"\n",
    "        cytosol_segmentation:\n",
    "            model: \"cyto2\"\n",
    "        chunk_size: 50\n",
    "        match_masks: True\n",
    "        filtering_threshold_mask_matching: 0.95\n",
    "        filter_masks_size: False\n",
    "    TimecourseHDF5CellExtraction:\n",
    "        compression: True\n",
    "        threads: 80 # threads used in multithreading\n",
    "        image_size: 128 # image size in pixel\n",
    "        cache: \".\"\n",
    "        hdf5_rdcc_nbytes: 5242880000 # 5gb 1024 * 1024 * 5000 \n",
    "        hdf5_rdcc_w0: 1\n",
    "        hdf5_rdcc_nslots: 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1d0b3-c027-4827-b8c2-d09c944d9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize project\n",
    "project_location = \"../example_projects/example_5/project\"\n",
    "\n",
    "project = TimecourseProject(\n",
    "    project_location,\n",
    "    config_path=\"../example_projects/example_5/config_example5.yml\",\n",
    "    segmentation_f=Multithreaded_Cytosol_Cellpose_TimecourseSegmentation,\n",
    "    extraction_f=TimecourseHDF5CellExtraction,\n",
    "    overwrite=False,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de66e978-e393-40e8-a4ce-2b12cd264a45",
   "metadata": {},
   "source": [
    "## Input Data Format\n",
    "\n",
    "scPortrait was optimized to work with Data generated using an Opera Phenix Microscope. Each Project class has several data_loader functions associated with it that can be used to load data from different sources. You can find the code under `src/scportrait/project.py`. The Dataloaders that load images from files are optimzied for file_name notation generated by the Phenix and the associated processing scripts. \n",
    "\n",
    "Data Format for `load_input_from_files`:\n",
    "\n",
    "    .... main directory\n",
    "        - Row02_Well02_Row02_Well02\n",
    "            - Timepoint001_Row02_Well02_Alexa488_zstack001Row02_Well02.tif\n",
    "            - Timepoint001_Row02_Well02_mCherry_zstack001_r000_c000.tif\n",
    "            - Timepoint001_Row02_Well02_DAPI_zstack001_r000_c000.tif\n",
    "        - ...\n",
    "\n",
    "Alternatively you can load data from a numpy array using `load_input_from_array` which can be used more flexibly to write your own dataloader:\n",
    "\n",
    "- images needs to be supplied in the shape (N, C, X, Y) to the parameter img\n",
    "- labels need to be supplied as a tidy dataframe to the parameter label. The first two columns of the dataframe need to contian the following information: row_index, unique Image ID. Then you can append as many additional labels as you want. \n",
    "\n",
    "Also feel free to submit a PR with a new dataloader for a common microscopy file format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb8411-2a7a-41bc-af84-f4aac9371b22",
   "metadata": {},
   "source": [
    "## Example Load data using the from files function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76698077-3116-4408-b4df-0deec9b4da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1\n",
    "channels = [\"DAPI\", \"Alexa488\", \"mCherry\"]\n",
    "timepoints = [\"Timepoint\" + str(x).zfill(3) for x in list(range(1, 3))]\n",
    "input_dir = \"../example_projects/example_5/input_images\"\n",
    "plate_layout = \"../example_projects/example_5/plate_layout.tsv\"\n",
    "\n",
    "project.load_input_from_files(\n",
    "    input_dir=input_dir, channels=channels, timepoints=timepoints, plate_layout=plate_layout, overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25970da-6c64-488d-b2a1-8f61705df688",
   "metadata": {},
   "source": [
    "Images and labels are written to the `input_segmentation.h5` HDF5 file contained under segmentation in the project folder.\n",
    "The input images are written to the container \"input_images\" and the labels to \"labels\". Column names are saved in a seperate dataset with the name \"label_names\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e66af-9fc2-4430-88b8-bfbbb620891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(f\"{project_location}/segmentation/input_segmentation.h5\") as hf:\n",
    "    print(hf.get(\"input_images\"))\n",
    "    print(hf.get(\"labels\"))\n",
    "\n",
    "    print(\"Example Label Dataset\")\n",
    "    print(hf.get(\"label_names\")[:])\n",
    "    print(hf.get(\"labels\")[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4f3dc-86f4-4319-b2a8-9a51b91c7b00",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "scPortrait has different segmentation workflows between which you can choose. If you run a timecourse project you will need to also select a timecourse segementation method. \n",
    "\n",
    "**notes on cellpose segmentation methods:**\n",
    "\n",
    "Cellpose currently does not allow for the processing of image batches. Each image will thus be segmented individually on the GPU. In case you have large images this isnt an issue since the GPU is still fully utilzed (the sharding parameter can be used to set an optimal shard size for maximizing GPU utilization). For a large quantity of small images this leads to a very inefficient processing. To improve the speed a bit you can try running with multiple threads. Each thread will also load the model though so it requires quite some Memory overhead. This is hopefully an issue we can address in future releases. If its something you would like to work on get in touch with the developers we are happy for help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658cd46-e514-4d51-b135-bde0020d3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.segment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b5be33-501e-4047-898f-2f976ed3738e",
   "metadata": {},
   "source": [
    "Segmentation results are written to the `input_segmentation.h5` HDF5 file in a new data container called \"segmentation\". \n",
    "In addition a csv file called classes.csv is generated which contains all the cell_ids that are not located in image edges and will be further processed.\n",
    "\n",
    "If we look at the segmentation dataset we can see that it contains a numpy array containing two segmentation masks for each input image: the nuclear segmentation and the cytosol segmentation generated by our chosen segmentation method. If you use the method CytosolOnly both of the masks will be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac930d-ae5c-4e4b-a0e6-36947fd2e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seg_overlay_timecourse(project_location, channel_to_show=0)\n",
    "plot_seg_overlay_timecourse(project_location, channel_to_show=1)\n",
    "\n",
    "plot_seg_overlay_timecourse(project_location, channel_to_show=0, seg_mask_to_show=1)\n",
    "plot_seg_overlay_timecourse(project_location, channel_to_show=1, seg_mask_to_show=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16ee3b-c3e3-478d-937f-c8e7764a6ce5",
   "metadata": {},
   "source": [
    "## Single-Cell Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2d1b0-f9d1-455c-a7a9-28434441c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27e89d-7e9a-42e3-aff5-b4e0fb0a5897",
   "metadata": {},
   "source": [
    "The extracted single-cell images are written to a h5py file single_cells.h5 located under `extraction\\data` within the project folder.\n",
    "\n",
    "The file contains four datasets: `single_cell_data`, `single_cell_index`, `single_cell_index_labelled`, `label_names`. \n",
    "\n",
    "`single_cell_data` contains the extracted single cell images while `single_cell_index` contains the cell id of the extracted cell at that location.\n",
    "\n",
    "The other two datacontainers integrated the labels from the base dataset and attach them to each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38171e2-3de8-4aec-b59f-4ccf37172985",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(f\"{project_location}/extraction/data/single_cells.h5\") as hf:\n",
    "    print(hf.keys())\n",
    "\n",
    "    print(hf.get(\"label_names\")[:])\n",
    "    print(hf.get(\"single_cell_index_labelled\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfbc56-6356-42cb-b6a5-a256bba01b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single cell index contains two columns, the first is the index position in the hdf5 datacontainer, the second is the cell id\n",
    "\n",
    "with h5py.File(f\"{project_location}/extraction/data/single_cells.h5\") as hf:\n",
    "    # print(hf[\"single_cell_index\"][0:10])\n",
    "    print(hf[\"single_cell_index\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98bc0b6-799e-4926-96f2-56d0b0c7b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single cell index labelled contains the same information as single_cell_index\n",
    "# in the first two columns but adds additional columns with labelling information, the column names for\n",
    "# the labelling information is contained in label_names\n",
    "\n",
    "with h5py.File(f\"{project_location}/extraction/data/single_cells.h5\") as hf:\n",
    "    print(hf[\"label_names\"][:])\n",
    "    print(hf[\"single_cell_index_labelled\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd824f9-c6a7-41a7-92d0-878963a5a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_single_cells(project_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scPortrait",
   "language": "python",
   "name": "scportrait"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
