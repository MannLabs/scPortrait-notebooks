{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A walk through the scPortrait Ecosystem\n",
    "\n",
    "This notebook will introduce you to the scPortrait ecosystem and give you a complete working example of how to use scPortrait. It will walk you through the following steps of the scPortrait workflow:\n",
    "1. **segmentation**: Generates masks for the segmentation of input images into individual cells\n",
    "2. **extraction**: The segmentation masks are applied to extract single-cell images for all cells in the input images. Images of individual cells are rescaled to [0, 1] per channel.\n",
    "3. **classification**: The image-based phenotype of each individual cell in the extracted single-cell dataset is classified using the specified classification method. Multiple classification runs can be performed on the same dataset using different classification methods. Here we utilize the pretrained binary classifier from the original [SPARCS manuscript](https://doi.org/10.1101/2023.06.01.542416) that identifies individual cells defective in a biological process called \"autophagy\". \n",
    "4. **selection**: Cutting instructions for the isolation of selected individual cells by laser microdissection are generated. The cutting shapes are written to an ``.xml`` file which can be loaded on a leica LMD microscope for automated cell excision.\n",
    "\n",
    "The data used in this notebook was previously stitched using the stitching workflow in [SPARCStools](https://github.com/MannLabs/SPARCStools). Please see the notebook [here](https://mannlabs.github.io/SPARCStools/html/pages/notebooks/example_stitching_notebook.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First we need to import all of the python functions we require to run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scportrait.pipeline.classification import MLClusterClassifier\n",
    "from scportrait.pipeline.extraction import HDF5CellExtraction\n",
    "from scportrait.pipeline.project import Project\n",
    "from scportrait.pipeline.segmentation.workflows import WGASegmentation\n",
    "from scportrait.pipeline.selection import LMDSelection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Project Structure\n",
    "\n",
    "Executing this code will generate a new scPortrait project in the designated location. Each scPortrait project has the following general structure:\n",
    "\n",
    "                .\n",
    "                ├── classification\n",
    "                │   └── classifier_name\n",
    "                │       └── processing.log\n",
    "                ├── config.yml\n",
    "                ├── extraction\n",
    "                ├── segmentation\n",
    "                └── processing.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_location = \"../../../example_data/example_1/project\"\n",
    "\n",
    "project = Project(\n",
    "    os.path.abspath(project_location),\n",
    "    config_path=\"../../../example_data/example_1/config_example1.yml\",\n",
    "    overwrite=True,\n",
    "    debug=True,\n",
    "    segmentation_f=WGASegmentation,\n",
    "    extraction_f=HDF5CellExtraction,\n",
    "    classification_f=MLClusterClassifier,\n",
    "    selection_f=LMDSelection,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Imaging Data\n",
    "\n",
    "Once we have generated our project structure we next need to load our imaging data. There are several different ways to do this. \n",
    "\n",
    "1. you can load the images directly from file by specifying a list of filepaths\n",
    "2. you can load the images from numpy arrays that are already loaded into memory\n",
    "\n",
    "Here it is important that you load the channels in the following order: Nucleus, Cellmembrane, others\n",
    "\n",
    "In this particular example we are utilizing images from U2OS cells which are stained with Hoechst 33342 to visualize nuclei and express Lck-mNeon to visualize the cellular membrane and LC3B-mcherry, a fluroescently tagged protein relevant for visualizing the biological process of autophagy. The cells have not been stimulated to induce autophagy and should be autophagy defective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: loading images from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    \"../../../example_data/example_1/input_images/Ch1.tif\",\n",
    "    \"../../../example_data/example_1/input_images/Ch2.tif\",\n",
    "    \"../../../example_data/example_1/input_images/Ch3.tif\",\n",
    "]\n",
    "\n",
    "project.load_input_from_tif_files(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: loading images from numpy array\n",
    "\n",
    "To simulate the case where the images you want to load are already loaded as a numpy array, we first convert the images to a numpy array and then pass this array to the project instead of only providing the image paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread\n",
    "\n",
    "images = [\n",
    "    \"../../../example_data/example_1/input_images/Ch1.tif\",\n",
    "    \"../../../example_data/example_1/input_images/Ch2.tif\",\n",
    "    \"../../../example_data/example_1/input_images/Ch3.tif\",\n",
    "]\n",
    "\n",
    "image_arrays = np.array([imread(path) for path in images])\n",
    "\n",
    "project.load_input_from_array(image_arrays)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the loaded images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded images are accesible via the input_image parameter of the project. They are saved into the underlying spatialdata object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize input images as example\n",
    "# it is not recommended to execute this block with large input images as it will compute for some time\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 6))\n",
    "axs[0].imshow(project.input_image[0])\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"nuclear stain: Hoechst 33342\")\n",
    "\n",
    "axs[1].imshow(project.input_image[1])\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"cellmembrane stain: Lck-mNeon\")\n",
    "\n",
    "axs[2].imshow(project.input_image[2])\n",
    "axs[2].axis(\"off\")\n",
    "axs[2].set_title(\"Stain of interest 1: mcherry tagged LC3B\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively you can also visualize the input images as well as all other objects saved in spatialdata object\n",
    "\n",
    "project.view_sdata()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Segmentation\n",
    "\n",
    "scPortrait has different segmentation workflows between which you can choose. If none of them fit your needs you can also easily write your own. \n",
    "\n",
    "Here we will demonstrate the CPU based classical segmentation method that was also utilized in the manuscript. \n",
    "\n",
    "We define the segmentation method to be used when we initialize the project. In this case we used the `WGASegmentation` method. The `ShardedWGASegmentation` works very similarily but allows you to process several image chunks in parallel for more efficient computation on large input images.\n",
    "\n",
    "By specifying `debug = True` we can see intermediate output results from the segmentation. \n",
    "\n",
    "The `WGASegmentation` method relies on parameters specified in the `config.yml` we loaded when initializing our project.\n",
    "\n",
    "        WGASegmentation:\n",
    "        input_channels: 3\n",
    "        cache: \".\"\n",
    "        chunk_size: 50 # chunk size for chunked HDF5 storage\n",
    "        lower_quantile_normalization:   0.001\n",
    "        upper_quantile_normalization:   0.999\n",
    "        median_filter_size:   4 # Size in pixels\n",
    "        nucleus_segmentation:\n",
    "            lower_quantile_normalization:   0.01 # quantile normalization of dapi channel before local tresholding Strong normalization (0.05,0.95) can help with nuclear speckles.\n",
    "            upper_quantile_normalization:   0.99 # quantile normalization of dapi channel before local tresholding.Strong normalization (0.05,0.95) can help with nuclear speckles.\n",
    "            median_block: 41 # Size of pixel disk used for median, should be uneven\n",
    "            median_step: 4\n",
    "            threshold: 0.2 # threshold above local median for nuclear segmentation\n",
    "            min_distance: 8 # minimum distance between two nucleis in pixel\n",
    "            peak_footprint: 7 # \n",
    "            speckle_kernel: 9 # Erosion followed by Dilation to remove speckels, size in pixels, should be uneven\n",
    "            dilation: 0 # final dilation of pixel mask       \n",
    "            min_size: 200 # minimum nucleus area in pixel\n",
    "            max_size: 1000 # maximum nucleus area in pixel\n",
    "            contact_filter: 0.5 # minimum nucleus contact with background\n",
    "        cytosol_segmentation:\n",
    "            threshold: 0.05 # treshold above which cytosol is detected\n",
    "            lower_quantile_normalization: 0.01\n",
    "            upper_quantile_normalization: 0.99\n",
    "            erosion: 2 # erosion and dilation are used for speckle removal and shrinking / dilation\n",
    "            dilation: 7 # for no change in size choose erosion = dilation, for larger cells increase the mask erosion\n",
    "            min_clip: 0\n",
    "            max_clip: 0.2\n",
    "            min_size: 200\n",
    "            max_size: 6000\n",
    "        chunk_size: 50\n",
    "        filter_masks_size: True\n",
    "\n",
    "By passing the parameter ``debug = True`` we tell scPortrait to also generate intermediate outputs which we can look at to see the different segmentation steps. It is only recommended to do this for debugging or visualization purposes as it will utilize more memory and be slower.\n",
    "\n",
    "For the ``WGASegmentation`` method the intermediate outputs that are displayed are the following: \n",
    "\n",
    "1. percentile normalized input images for each of the three channels (3 images)\n",
    "2. median normalized input images (this slightl smooths the images for a better segmentation result) (3 images)\n",
    "3. histogram showing the intensity distribution for nuclear and cytosolic channel (2 plots)\n",
    "4. generated mask after applying nuclear thresholding\n",
    "5. nuclear thresholding mask with calculated centers for each detected nucleus\n",
    "6. fast marching map with nuclei centers indicated in red\n",
    "7. segmented individual nuclei (2 different visualizations)\n",
    "8. starting nucleus mask for filtering\n",
    "9. histrogram showing size distribution of segmented nuclei\n",
    "10. segmentation mask with too small nuclei shown in red\n",
    "11. segmentation mask with too large nuclei shown in red\n",
    "12. WGA thresholding mask\n",
    "13. WGA potential map\n",
    "14. WGA fast marching results\n",
    "15. Watershed results with nuclear centers shown in red\n",
    "15. WGA mask\n",
    "16. Cytosol size distribution\n",
    "10. cytosol mask with too small cells shown in red\n",
    "11. cytosol mask with too large cells shown in red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.segment()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Segmentation Results\n",
    "\n",
    "The Segmentation Results are written to a hdf5 file called `segmentation.h5` located in the segmentation directory of our scPortrait project.\n",
    "\n",
    "The file contains two datasets: `channels` and `labels`. Channels contains the input images and `labels` the generated segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(project.sdata.labels[\"seg_all_nucleus\"])\n",
    "plt.figure()\n",
    "plt.imshow(project.sdata.labels[\"seg_all_cytosol\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the `labels` dataset we can see that it contains a numpy array containing two segmentation masks: the nuclear segmentation and the cytosol segmentation generated by our chosen segmentation method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each segmentation mask is an array where each pixel is assigned either to background (`0`) or to a specific cell (`cellid: whole number`).\n",
    "\n",
    "If we zoom in on the corner of the segmentation mask of a nucleus the numpy array would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(project.sdata.labels[\"seg_all_nucleus\"][230:250, 945:955])\n",
    "plt.axis(\"off\")\n",
    "print(project.sdata.labels[\"seg_all_nucleus\"][230:250, 945:955].data.compute())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting single-cell images\n",
    "\n",
    "Once we have generated a segmentation mask, the next step is to extract single-cell images of segmented cells in the dataset.\n",
    "\n",
    "Like during the segmentation there are several extraction methods to choose from. For regular scPortrait projects we need the `HDF5CellExtraction` method. This will extract single-cell images for all cells segmentated in the dataset and write them to a hdf5 file.\n",
    "\n",
    "The parameters with which `HDF5CellExtraction` will run are again specified in the `config.yml` file.\n",
    "\n",
    "                HDF5CellExtraction:\n",
    "                    compression: True\n",
    "                    threads: 80 # threads used in multithreading\n",
    "                    image_size: 128 # image size in pixel\n",
    "                    cache: \".\"\n",
    "                    hdf5_rdcc_nbytes: 5242880000 # 5gb 1024 * 1024 * 5000 \n",
    "                    hdf5_rdcc_w0: 1\n",
    "                    hdf5_rdcc_nslots: 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.extract()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at extracted single-cell images\n",
    "\n",
    "The extracted single-cell images are written to a h5py file `single_cells.h5` located under `extraction\\data` within the project folder.\n",
    "\n",
    "The file contains two datasets: `single_cell_data` and `single_cell_index`. `single_cell_data` contains the extracted single cell images while `single_cell_index` contains the cell id of the extracted cell at that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(f\"{project_location}/extraction/data/single_cells.h5\") as hf:\n",
    "    print(hf.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we want to look at the nth cell in the dataset we can first check which cellid was assigned to this cell by looking at the nth entry in the `single_cell_index` dataset and then get the extracted single-cell images from the `single_cell_data` dataset.\n",
    "\n",
    "The extracted single-cell images represent the following information in this order:  \n",
    "<ol>\n",
    "1. nucleus mask<br>\n",
    "                2. cytosol mask  <br>\n",
    "                3. nucleus channel <br> \n",
    "                4. cytosol channel <br> \n",
    "                5. other channels<br>\n",
    "</ol> \n",
    "\n",
    "Here we will demonstrate with the 10th cell in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(f\"{project_location}/extraction/data/single_cells.h5\") as hf:\n",
    "    index = hf.get(\"single_cell_index\")\n",
    "    images = hf.get(\"single_cell_data\")\n",
    "\n",
    "    print(\"cell id: \", index[10])\n",
    "\n",
    "    image = images[10]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 5)\n",
    "\n",
    "    for i, _img in enumerate(image):\n",
    "        axs[i].imshow(_img)\n",
    "        axs[i].axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of extracted single-cells\n",
    "\n",
    "Next we can apply a pretained model to classify our cells within the scPortrait project. \n",
    "\n",
    "Within the `config.yml` we specify which model should be used for inference and we can give it a name. \n",
    "\n",
    "                MLClusterClassifier:\n",
    "                    channel_classification: 4\n",
    "                    threads: 24 #\n",
    "                    batch_size: 900\n",
    "                    dataloader_worker: 0 #needs to be 0 if using cpu\n",
    "                    standard_scale: False\n",
    "                    exp_transform: False\n",
    "                    log_transform: False\n",
    "                    network: \"autophagy_classifier2.1\"\n",
    "                    classifier_architecture: \"VGG2_old\"\n",
    "                    screen_label: \"Autophagy_15h_classifier2.1\"\n",
    "                    epoch: \"max\"\n",
    "                    encoders: [\"forward\"]\n",
    "                    inference_device: \"cpu\"\n",
    "\n",
    "Here e.g. we are using a pretrained model included within the scPortrait library `autophagy_classifier2.1` and are naming the results from this model `Autophagy_15h_classifier2.1`. \n",
    "\n",
    "Model overview:\n",
    "\n",
    "<img src=\"../images/classifying_autophagy.png\" alt=\"autophagy classification with example cells\" width=\"800\"/>\n",
    "\n",
    "\n",
    "The inference results will be written to a new folder generated under `classification` with this name. \n",
    "\n",
    "If we want to use a model we trained ourselves that is not yet included within the scPortrait library we can simply replace the network name in the config with the path to the checkpoint file generated by pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.classify()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at the generated results\n",
    "\n",
    "The results are written to a csv file which we can load with pandas.\n",
    "\n",
    "scPortrait writes the softmax results directly to csv as `ln()` for better precision. To transform these numbers back to the range between 0 and 1 we first need to apply the `np.exp` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\n",
    "    f\"{project_location}/classification/complete_Autophagy_15h_classifier2_1/inference_forward.csv\", index_col=None\n",
    ")\n",
    "results.result_0 = np.exp(results.result_0)\n",
    "results.result_1 = np.exp(results.result_1)\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the model we used what `result_0` and `result_1` represent will vary. Here we used a binary classification model were class 1 was cells deficient in autophagy. So `result_1` indicates the probability score that a cell has the label \"autophagy off\". `results_0` is simply `1 - result_1`\n",
    "\n",
    "\n",
    "If we look at the distribution in our dataset we can see that almost all cells are classified as \"autophagy defective\". Since the input images were from unstimulated wt cells this matches to our expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(results.result_1, bins=100)\n",
    "plt.title(\"Prob(Unstim)\")\n",
    "plt.xlabel(\"Classification Score\")\n",
    "plt.ylabel(\"Count\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Cutting contours for excision on the LMD7\n",
    "\n",
    "scPortrait directly interfaces with our other open-source python library [py-lmd](https://github.com/MannLabs/py-lmd) to easily select and export cells for excision on a Leica LMD microscope. \n",
    "\n",
    "Of note: this will require that the cells have been plates on a LMD compatible surface (like a PPS slide). scPortrait can of course also simply be used for data analysis procedures, then ignore this last step.\n",
    "\n",
    "First we will select the cells we wish to excise based on their classification score. Here we have chosen a threadhold >= 0.99999 for bin1 and a threshold <= 0.9 for bin2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_ids_bin1 = results[results.result_1 >= 0.99999].cell_id.tolist()\n",
    "cell_ids_bin2 = results[results.result_1 <= 0.9].cell_id.tolist()\n",
    "\n",
    "print(\"number of cells to excise:\", len(cell_ids_bin1) + len(cell_ids_bin2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells can then be allocated into different wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_to_select = [\n",
    "    {\"name\": \"bin1\", \"classes\": list(cell_ids_bin1), \"well\": \"A1\"},\n",
    "    {\"name\": \"bin2\", \"classes\": list(cell_ids_bin2), \"well\": \"B1\"},\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to defining which cells we want to excise, we also need to pass the location of the calibration crosses so that we can transfer the image coordinate system into a cutting coordinate system. You can read up on this [here](https://mannlabs.github.io/py-lmd/html/pages/segmentation_loader.html#different-coordinate-systems).\n",
    "\n",
    "To obtain the coordinates of your reference points simply open your stitched image in e.g. FIJI and navigate to the location of your reference points. Then write out the coordinates for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_0 = (0, 0)\n",
    "marker_1 = (2000, 0)\n",
    "marker_2 = (0, 2000)\n",
    "\n",
    "calibration_marker = np.array([marker_0, marker_1, marker_2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous methods, additional parameters can be passed to the selection function via the `config.yml` file which adapts the behaviour of how cutting shapes are generated. You can read up more on this [here](https://mannlabs.github.io/py-lmd/html/pages/segmentation_loader.html#overview-of-configuration).\n",
    "\n",
    "                LMDSelection:\n",
    "                    processes: 20\n",
    "\n",
    "                    # defines the channel used for generating cutting masks\n",
    "                    # segmentation.hdf5 => labels => segmentation_channel\n",
    "                    # When using WGA segmentation:\n",
    "                    #    0 corresponds to nuclear masks\n",
    "                    #    1 corresponds to cytosolic masks.\n",
    "                    segmentation_channel: 0\n",
    "\n",
    "                    # dilation of the cutting mask in pixel\n",
    "                    shape_dilation: 16\n",
    "\n",
    "                    # number of datapoints which are averaged for smoothing\n",
    "                    # the number of datapoints over an distance of n pixel is 2*n\n",
    "                    smoothing_filter_size: 25\n",
    "\n",
    "                    # fold reduction of datapoints for compression\n",
    "                    poly_compression_factor: 30\n",
    "                    \n",
    "                    # can be \"none\", \"greedy\", or \"hilbert\"\n",
    "                    path_optimization: \"hilbert\"\n",
    "                    \n",
    "                    # number of nearest neighbours for optimized greedy search\n",
    "                    greedy_k: 15\n",
    "                    \n",
    "                    # hilbert curve order\n",
    "                    hilbert_p: 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.select(cells_to_select, calibration_marker, segmentation_name=\"seg_all_nucleus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scPortrait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
